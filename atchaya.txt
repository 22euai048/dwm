
csv   

import pandas as pd
import numpy as np

# Define the dataset based on the provided image
data = {
    'Day': ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14'],
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],
    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

# Convert the dataset into a pandas DataFrame
df = pd.DataFrame(data)


df.to_csv('file2.csv',index=False)




























1)decision tree

"""import pandas as pd
import numpy as np

# Define the dataset based on the provided image
data = {
    'Day': ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14'],
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],
    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

# Convert the dataset into a pandas DataFrame
df = pd.DataFrame(data)

# Save the DataFrame to a CSV file
csv_file_path = '/mnt/data/play_tennis.csv'
df.to_csv(csv_file_path, index=False)

# Load the data back from CSV
data = pd.read_csv(csv_file_path).values

# Calculate entropy of a dataset
def entropy(y):
    from collections import Counter
    counts = Counter(y)
    probabilities = [count / len(y) for count in counts.values()]
    return -sum(p * np.log2(p) for p in probabilities)

# Calculate information gain of a split
def information_gain(y, y_left, y_right):
    p = len(y_left) / len(y)
    return entropy(y) - p * entropy(y_left) - (1 - p) * entropy(y_right)

# Split the dataset
def split_dataset(dataset, feature_index, value):
    left_split = np.array([row for row in dataset if row[feature_index] == value])
    right_split = np.array([row for row in dataset if row[feature_index] != value])
    return left_split, right_split

# Find the best split
def best_split(dataset):
    best_gain = 0
    best_feature_index = 0
    best_value = None
    n_features = len(dataset[0]) - 1
    for feature_index in range(n_features):
        values = set(row[feature_index] for row in dataset)
        for value in values:
            left, right = split_dataset(dataset, feature_index, value)
            if len(left) == 0 or len(right) == 0:
                continue
            y, y_left, y_right = dataset[:, -1], left[:, -1], right[:, -1]
            gain = information_gain(y, y_left, y_right)
            if gain > best_gain:
                best_gain, best_feature_index, best_value = gain, feature_index, value
    return best_feature_index, best_value

# Create the decision tree
class DecisionTreeNode:
    def __init__(self, feature_index=None, value=None, left=None, right=None, label=None):
        self.feature_index = feature_index
        self.value = value
        self.left = left
        self.right = right
        self.label = label

def build_tree(dataset):
    y = dataset[:, -1]
    if len(set(y)) == 1:
        return DecisionTreeNode(label=y[0])
    feature_index, value = best_split(dataset)
    if feature_index is None:
        return DecisionTreeNode(label=max(set(y), key=list(y).count))
    left, right = split_dataset(dataset, feature_index, value)
    left_branch = build_tree(left)
    right_branch = build_tree(right)
    return DecisionTreeNode(feature_index=feature_index, value=value, left=left_branch, right=right_branch)

# Print the decision tree
def print_tree(node, spacing=""):
    if node.label is not None:
        print(spacing + "Predict:", node.label)
        return
    print(spacing + f"[{feature_names[node.feature_index]} == {node.value}]")
    print(spacing + '--> True:')
    print_tree(node.left, spacing + "  ")
    print(spacing + '--> False:')
    print_tree(node.right, spacing + "  ")

# Feature names for printing
feature_names = ['Outlook', 'Temperature', 'Humidity', 'Wind']

# Build and print the tree
tree = build_tree(data)
print_tree(tree)
"""
import pandas as pd
import numpy as np



# Load the data back from CSV
data = pd.read_csv(r'C:\Users\ADMIN\Downloads\python.py\file2.csv').values

# Calculate entropy of a dataset
def entropy(y):
    from collections import Counter
    counts = Counter(y)
    probabilities = [count / len(y) for count in counts.values()]
    return -sum(p * np.log2(p) for p in probabilities)

# Calculate information gain of a split
def information_gain(y, y_left, y_right):
    p = len(y_left) / len(y)
    return entropy(y) - p * entropy(y_left) - (1 - p) * entropy(y_right)

# Split the dataset
def split_dataset(dataset, feature_index, value):
    left_split = np.array([row for row in dataset if row[feature_index] == value])
    right_split = np.array([row for row in dataset if row[feature_index] != value])
    return left_split, right_split

# Find the best split
def best_split(dataset):
    best_gain = 0
    best_feature_index = 0
    best_value = None
    n_features = len(dataset[0]) - 1
    for feature_index in range(n_features):
        values = set(row[feature_index] for row in dataset)
        for value in values:
            left, right = split_dataset(dataset, feature_index, value)
            if len(left) == 0 or len(right) == 0:
                continue
            y, y_left, y_right = dataset[:, -1], left[:, -1], right[:, -1]
            gain = information_gain(y, y_left, y_right)
            if gain > best_gain:
                best_gain, best_feature_index, best_value = gain, feature_index, value
    return best_feature_index, best_value

# Create the decision tree
class DecisionTreeNode:
    def __init__(self, feature_index=None, value=None, left=None, right=None, label=None):
        self.feature_index = feature_index
        self.value = value
        self.left = left
        self.right = right
        self.label = label

def build_tree(dataset):
    y = dataset[:, -1]
    if len(set(y)) == 1:
        return DecisionTreeNode(label=y[0])
    feature_index, value = best_split(dataset)
    if feature_index is None:
        return DecisionTreeNode(label=max(set(y), key=list(y).count))
    left, right = split_dataset(dataset, feature_index, value)
    left_branch = build_tree(left)
    right_branch = build_tree(right)
    return DecisionTreeNode(feature_index=feature_index, value=value, left=left_branch, right=right_branch)

# Print the decision tree
def print_tree(node, spacing=""):
    if node.label is not None:
        print(spacing + "Predict:", node.label)
        return
    print(spacing + f"[{feature_names[node.feature_index]} == {node.value}]")
    print(spacing + '--> True:')
    print_tree(node.left, spacing + "  ")
    print(spacing + '--> False:')
    print_tree(node.right, spacing + "  ")

# Feature names for printing
feature_names = ['Outlook', 'Temperature', 'Humidity', 'Wind']

# Build and print the tree
tree = build_tree(data)
print_tree(tree)

--------------------------------------------------------------------------------------------------------------
3 regression

"""import numpy as np
import matplotlib.pyplot as plt

# Given data points
x = [0, 1, 2, 3, 4]
y = [2, 3, 5, 4, 6]

# Step 1: Calculate the means of x and y
x_mean = sum(x) / len(x)
y_mean = sum(y) / len(y)

# Step 2: Calculate the coefficients a and b
# Calculate the numerator and denominator for a
numerator = sum((x[i] - x_mean) * (y[i] - y_mean) for i in range(len(x)))
denominator = sum((x[i] - x_mean) ** 2 for i in range(len(x)))
a = numerator / denominator
b = y_mean - a * x_mean

print(f"The linear regression line is y = {a:.2f}x + {b:.2f}")

# Step 3: Estimate the value of y when x = 10
x_new = 10
y_new = a * x_new + b
print(f"The estimated value of y when x = {x_new} is {y_new:.2f}")

# Step 4: Calculate the Mean Squared Error (MSE)
y_pred = [a * xi + b for xi in x]
mse = sum((y[i] - y_pred[i]) ** 2 for i in range(len(y))) / len(y)
print(f"The Mean Squared Error of the model is {mse:.2f}")

# Plot the data points and the regression line
plt.scatter(x, y, color='blue', label='Data points')
plt.plot(x, y_pred, color='red', label='Regression line')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()


#third question , no built in functions"""
import numpy as np

# Data
x = np.array([0, 1, 2, 3, 4])
y = np.array([2, 3, 5, 4, 6])

# Linear regression calculations
n = len(x)
x_mean = np.mean(x)
y_mean = np.mean(y)

b1_num = np.sum((x - x_mean) * (y - y_mean))
b1_den = np.sum((x - x_mean) ** 2)
b1 = b1_num / b1_den
b0 = y_mean - b1 * x_mean

# Prediction
x_new = 10
y_new = b0 + b1 * x_new

# Error calculation
y_pred = b0 + b1 * x
error = np.mean((y - y_pred) ** 2)

print(f"Linear regression line: y = {b1:.2f}x + {b0:.2f}")
print(f"Estimated value of y when x = 10: {y_new:.2f}")
print(f"Mean squared error: {error:.2f}")

-----------------------------------------------------------------------------------

4 cluster

# 4th q
import numpy as np
import matplotlib.pyplot as plt

# Given points
points = np.array([
    [2, 10],
    [2, 5],
    [8, 4],
    [5, 8],
    [7, 5],
    [6, 4],
    [1, 2],
    [4, 9]
])

# Initial cluster centers
initial_centers = np.array([
    [2, 10],
    [5, 8],
    [1, 2]
])

# Function to compute the Euclidean distance
def euclidean_distance(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

# Function to assign points to the nearest cluster center
def assign_clusters(points, centers):
    clusters = {}
    for i in range(len(centers)):
        clusters[i] = []
    for point in points:
        distances = [euclidean_distance(point, center) for center in centers]
        cluster = distances.index(min(distances))
        clusters[cluster].append(point)
    return clusters

# Function to recalculate the cluster centers
def recalculate_centers(clusters):
    new_centers = []
    for cluster in clusters.values():
        new_center = np.mean(cluster, axis=0)
        new_centers.append(new_center)
    return new_centers

# K-means clustering algorithm
def kmeans(points, initial_centers, max_iterations=100):
    centers = initial_centers
    for _ in range(max_iterations):
        clusters = assign_clusters(points, centers)
        new_centers = recalculate_centers(clusters)
        if np.allclose(centers, new_centers):
            break
        centers = new_centers
    return centers, clusters

# Run the K-means algorithm
final_centers, clusters = kmeans(points, initial_centers)

print("Final cluster centers:")
for i, center in enumerate(final_centers):
    print(f"Cluster {i+1} center: {center}")

print("\nCluster assignments:")
for i, cluster in clusters.items():
    print(f"Cluster {i+1}: {cluster}")

# Convert final_centers to a numpy array
final_centers = np.array(final_centers)

# Plot the results
colors = ['r', 'g', 'b']
for i, cluster in clusters.items():
    cluster_points = np.array(cluster)
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], color=colors[i], label=f'Cluster {i+1}')
plt.scatter(final_centers[:, 0], final_centers[:, 1], color='k', marker='x', s=100, label='Centers')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('K-means Clustering')
plt.show()


--------------------------------------------------------------------------------------------------------

#6TH Q

import numpy as np
import matplotlib.pyplot as plt

# Given points
points = np.array([
    [2, 2],   # A
    [3, 2],   # B
    [1, 1],   # C
    [3, 1],   # D
    [1.5, 0.5] # E
])

# Initial cluster centers
initial_centers = np.array([
    [2, 2],   # Center for cluster 1
    [1, 1]    # Center for cluster 2
])

# Function to compute the Euclidean distance
def euclidean_distance(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

# Function to assign points to the nearest cluster center
def assign_clusters(points, centers):
    clusters = {}
    for i in range(len(centers)):
        clusters[i] = []
    for point in points:
        distances = [euclidean_distance(point, center) for center in centers]
        cluster = distances.index(min(distances))
        clusters[cluster].append(point)
    return clusters

# Function to recalculate the cluster centers
def recalculate_centers(clusters):
    new_centers = []
    for cluster in clusters.values():
        new_center = np.mean(cluster, axis=0)
        new_centers.append(new_center)
    return new_centers

# K-means clustering algorithm
def kmeans(points, initial_centers, max_iterations=100):
    centers = initial_centers
    for _ in range(max_iterations):
        clusters = assign_clusters(points, centers)
        new_centers = recalculate_centers(clusters)
        if np.allclose(centers, new_centers):
            break
        centers = new_centers
    return centers, clusters

# Run the K-means algorithm
final_centers, clusters = kmeans(points, initial_centers)

print("Final cluster centers:")
for i, center in enumerate(final_centers):
    print(f"Cluster {i+1} center: {center}")

print("\nCluster assignments:")
for i, cluster in clusters.items():
    print(f"Cluster {i+1}: {cluster}")

# Convert final_centers to a numpy array
final_centers = np.array(final_centers)

# Plot the results
colors = ['r', 'b']
labels = ['A', 'B', 'C', 'D', 'E']
for i, cluster in clusters.items():
    cluster_points = np.array(cluster)
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], color=colors[i], label=f'Cluster {i+1}')
    for j, point in enumerate(cluster_points):
        plt.text(point[0], point[1], labels[j], fontsize=12)
plt.scatter(final_centers[:, 0], final_centers[:, 1], color='k', marker='x', s=100, label='Centers')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('K-means Clustering')
plt.show()

-----------------------------------------------------------------------------------------------------------
5.  
# 5th Q
import pandas as pd
import numpy as np

# Create the dataset
data = {
    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],
    'Age': [21, 35, 26, 45, 67, np.nan, 32, 31, np.nan, 42, np.nan, 32, 35, 35],
    'Income': ['1L', '1,00,000', '45000', '', '10,000', '10000', '5$', '5 Dollars', '10,000', '15000', '25,000', '35000', '150000', '35000'],
    'Gender': ['Male', 'Male', 'Male', '', 'Female', 'Female', 'Female', 'Male', 'Female', 'Female', 'Female', 'Male', 'Female', 'Male'],
    'DoB': ['31.05.1992', '10-05-2002', 'Aug 5, 2000', '', '31.03.1986', '10/5/1987', '31.05.1992', '10-05-2002', 'Aug 5, 2000', 'Sep 12’2000', '31.03.1986', '10/5/1987', 'Sep 12’2000', '31.03.1986'],
    'Buys': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

df = pd.DataFrame(data)
print("Original Dataset:")
print(df)

# Handle missing data and formatting issues
# Replace '1L' with '100000' and '5$'/'5 Dollars' with '5000'
df['Income'] = df['Income'].replace(['1L', '5$', '5 Dollars'], ['100000', '5000', '5000'])

# Convert 'Income' to numeric, forcing errors to NaN and then filling missing values with the median
df['Income'] = pd.to_numeric(df['Income'].str.replace(',', ''), errors='coerce')
df['Income'].fillna(df['Income'].median(), inplace=True)

# Fill missing 'Age' values with the median
df['Age'].fillna(df['Age'].median(), inplace=True)

# Fill missing 'Gender' values with the mode7.
df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)

# Convert 'DoB' to datetime, forcing errors to NaT and then filling missing values with the mode
df['DoB'] = pd.to_datetime(df['DoB'], errors='coerce')
df['DoB'].fillna(df['DoB'].mode()[0], inplace=True)

print("\nCleaned Dataset:")
print(df)

# Apply five statistical measures
mean_age = df['Age'].mean()
median_income = df['Income'].median()
std_income = df['Income'].std()
count_gender = df['Gender'].value_counts()
mode_buys = df['Buys'].mode()[0]

---------------------------------------------------------------------------------------------------------------
7.

import pandas as pd
import itertools

# Data creation
data = {'TransactionID': ['T1', 'T2', 'T3', 'T4', 'T5', 'T6'],
        'Items': [['HotDogs', 'Buns', 'Ketchup'],
                  ['HotDogs', 'Buns'],
                  ['HotDogs', 'Coke', 'Chips'],
                  ['Chips', 'Coke'],
                  ['Chips', 'Ketchup'],
                  ['HotDogs', 'Coke', 'Chips']]}

df = pd.DataFrame(data)

# Creating the basket (one-hot encoded DataFrame)
basket = pd.DataFrame(df['Items'].tolist(), index=df['TransactionID']).stack().reset_index(level=1, drop=True).reset_index()
basket.columns = ['TransactionID', 'Items']
basket = pd.crosstab(basket['TransactionID'], basket['Items'])

# Encoding function
def encode_units(x):
    return 1 if x >= 1 else 0

basket_sets = basket.applymap(encode_units)

# Function to generate frequent itemsets
def apriori_manual(df, min_support=0.3334):
    itemset_support = {}
    num_transactions = len(df)
    items = df.columns

    def get_support(itemset):
        mask = df[list(itemset)].all(axis=1)
        return mask.sum() / num_transactions

    # Generate frequent 1-itemsets
    for item in items:
        support = get_support([item])
        if support >= min_support:
            itemset_support[frozenset([item])] = support

    current_itemsets = list(itemset_support.keys())
    k = 2

    while current_itemsets:
        new_itemsets = list(itertools.combinations(set(itertools.chain.from_iterable(current_itemsets)), k))
        new_itemset_support = {}

        for itemset in new_itemsets:
            support = get_support(itemset)
            if support >= min_support:
                new_itemset_support[frozenset(itemset)] = support

        itemset_support.update(new_itemset_support)
        current_itemsets = new_itemset_support.keys()
        k += 1

    frequent_itemsets = pd.DataFrame(
        [(list(itemset), support) for itemset, support in itemset_support.items()],
        columns=['itemsets', 'support']
    )

    return frequent_itemsets

frequent_itemsets = apriori_manual(basket_sets)

# Function to generate association rules
def generate_association_rules(frequent_itemsets, min_confidence=0.6):
    rules = []
    itemsets = frequent_itemsets['itemsets'].tolist()
    supports = dict(zip(map(frozenset, frequent_itemsets['itemsets']), frequent_itemsets['support']))

    for itemset in itemsets:
        if len(itemset) > 1:
            for subset in itertools.chain(*[itertools.combinations(itemset, r) for r in range(1, len(itemset))]):
                antecedent = frozenset(subset)
                consequent = frozenset(itemset) - antecedent
                if supports[frozenset(itemset)] / supports[antecedent] >= min_confidence:
                    rules.append({
                        'antecedent': list(antecedent),
                        'consequent': list(consequent),
                        'antecedent support': supports[antecedent],
                        'consequent support': supports[consequent],
                        'support': supports[frozenset(itemset)],
                        'confidence': supports[frozenset(itemset)] / supports[antecedent],
                        'lift': supports[frozenset(itemset)] / (supports[antecedent] * supports[consequent])
                    })

    return pd.DataFrame(rules)

rules = generate_association_rules(frequent_itemsets)

print("Frequent Itemsets:")
print(frequent_itemsets)
print("\nAssociation Rules:")
print(rules.sort_values(by='confidence', ascending=False))

